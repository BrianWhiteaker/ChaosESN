{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuda:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.8.12 (default, Oct 12 2021, 13:49:34) \n",
      "[GCC 7.5.0]\n",
      "Numpy: 1.21.2\n",
      "Torch: 1.10.0\n",
      "Signal length M=15000\n",
      "Normalizing value MM is [38.05303465 52.5343247  46.87228953]\n",
      "std = [7.9122292  8.99288317 8.56681415]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'/home/bwhiteak/ChaosESN/ESN_utils/')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import rc_tools as rct\n",
    "import rc_analysis as rca\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import pdb\n",
    "\n",
    "from skopt.space import Real,Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "#jtplot.style(theme='solarizedd', context='notebook', ticks=True, grid=False)\n",
    "torch.set_num_threads(4)\n",
    "np.random.seed(11)\n",
    "torch.set_printoptions(precision=10)\n",
    "dtype = torch.float32 \n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print(f'Numpy: {np.__version__}')\n",
    "print(f'Torch: {torch.__version__}')\n",
    "\n",
    "DEVICE = 'cuda:5'\n",
    "\n",
    "FREERUN = 20  # Extra predicition time-steps after test data\n",
    "deltaT = .02      # Number of extra free-running steps  int(20/.02) \n",
    "\n",
    "rho = 28.0\n",
    "sigma = 10.0\n",
    "beta = 8/3\n",
    "\n",
    "# Lorenz 1963\n",
    "def f(state, t):\n",
    "    x,y,z = state\n",
    "    return sigma*(y-x), x*(rho-z)-y, x*y - beta*z\n",
    "\n",
    "state0 = [1.,1.,1.]             # Initial point\n",
    "t = np.arange(0,300+FREERUN,deltaT)  # Total of 320 full steps with deltaT=.02\n",
    "states = odeint(f,state0,t)\n",
    "\n",
    "mu = np.mean(states, axis=0)       # Get mean for each of x,y,z\n",
    "signal = (states - mu)[:,[0,1,2]]  # Mean center the data\n",
    "M = signal.shape[0] - int(FREERUN/deltaT)  # Length of train plus test... no freerun\n",
    "K = 3                                  # Input dimension\n",
    "L = 3                                  # Output dimension\n",
    "RF = .5                                # For feedback <--- not implemented\n",
    "TEST = 1000                            # length of test\n",
    "LEAD = 100                            # Number of points to plot before test\n",
    "BURNIN = 100                           # Number of steps ignored for random x0 to fade\n",
    "REG = 1e-8                             # Regularization factor for ridge regression\n",
    "TRAINLENGTH = M-TEST    \n",
    "\n",
    "MINMAXS = np.max(signal[:TRAINLENGTH+TEST],axis=0)-np.min(signal[:TRAINLENGTH+TEST],axis=0)\n",
    "STD = np.std(signal[:TRAINLENGTH+TEST],axis=0)\n",
    "RGS = [(-19.5,19.5),(-27,27),(-25,25)]\n",
    "BINS = 50\n",
    "\n",
    "print(f'Signal length M={M}')\n",
    "print(f'Normalizing value MM is {MINMAXS}')\n",
    "print(f'std = {STD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matricesGPU(k,n,l,ri,ro):\n",
    "    win = torch.rand((n,k),dtype=dtype,\n",
    "                      device=torch.device(DEVICE)).sub(.5).mul(ri)\n",
    "    wfb = torch.zeros((n,l),dtype=dtype, device=torch.device(DEVICE))\n",
    "    wout = torch.rand((l,n+k),dtype=dtype,\n",
    "                      device=torch.device(DEVICE)).sub(.5).mul(ro)\n",
    "    return win, wfb, wout\n",
    "\n",
    "def set_vectorsGPU(n,l,r):\n",
    "    x0 = torch.rand((n,1),dtype=dtype,\n",
    "                      device=torch.device(DEVICE)).sub(.5).mul(r)\n",
    "    y0 = torch.zeros((l,1),dtype=dtype, device=torch.device(DEVICE))\n",
    "    return x0, y0\n",
    "\n",
    "def update_res_stateGPU(wnet,xt,uxy,a,g):\n",
    "    z = torch.matmul(wnet,uxy)\n",
    "    return torch.mul(xt,1-a) + torch.mul(torch.tanh(z),a*g)\n",
    "\n",
    "def predict_yGPU(wout,xu):\n",
    "    return torch.matmul(wout, xu)\n",
    "\n",
    "def get_matrixGPU(n,r,sr):\n",
    "    A = (torch.rand((n,n),dtype=dtype,\n",
    "                   device=torch.device(DEVICE))-.5)*r\n",
    "    At = torch.transpose(A,0,1)\n",
    "    D = torch.diag(torch.diag(A))   \n",
    "    W = A + At - D\n",
    "    eig = torch.eig(W, eigenvectors=False)\n",
    "    wsr = torch.max(torch.abs(eig[0]))\n",
    "    return W.div(wsr).mul(sr)\n",
    "\n",
    "def resize_spaces(mn, mx, best, isAlpha=False):\n",
    "    #pdb.set_trace()\n",
    "    if(best.size==0):\n",
    "        new_mn = np.max([0, mn - .5*mn])\n",
    "        new_mx = 1.5*mx\n",
    "    else:\n",
    "        best_mn = np.min(best)\n",
    "        best_mx = np.max(best)\n",
    "        mn_bound = (best_mn-mn)/2\n",
    "        mx_bound = (mx -best_mx)/2\n",
    "        new_mn, new_mx = best_mn-mn_bound, best_mx+mx_bound\n",
    "        print(f'\\nBest mn:{best_mn:.3f}\\t mn:{best_mx:.3f}')\n",
    "        print(f'New bounds mn--mx: {mn_bound:.3f}--{mx_bound:.3f}')\n",
    "    if(isAlpha):\n",
    "        if(new_mx>1):\n",
    "            new_mx = 1\n",
    "    \n",
    "    return new_mn, new_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop for gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "# numpy arrays cannot be written to json \n",
    "# convert to list\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self,obj):\n",
    "        if isinstance(obj,np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self,obj)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_a, max_a = .57, .6\n",
    "#min_sr, max_sr = 5.3, 5.4\n",
    "#min_g, max_g = .2, .24\n",
    "#min_ri, max_ri = .17, .2\n",
    "#min_rr, max_rr = 2.65, 2.7\n",
    "\n",
    "min_a, max_a = .5, .9\n",
    "min_sr, max_sr = 1.1, 1.7\n",
    "min_g, max_g = .2, .24\n",
    "min_ri, max_ri = .01, .1\n",
    "min_rr, max_rr = .85, 2.7\n",
    "space = [Real(min_a, max_a, name='a'),\n",
    "         Real(min_sr, max_sr, name='sr'),\n",
    "         Real(min_ri, max_ri, name='ri'),\n",
    "         Real(min_rr, max_rr, name='rr')\n",
    "        ]\n",
    "\n",
    "@use_named_args(space)\n",
    "def loop(a=1.0,sr=1.0,ri=1.0,rr=1.0):\n",
    "    start = time.time()\n",
    "    amp = 1.\n",
    "    global running_error, s, counter, signal, N, ref, rn, \\\n",
    "           alphas, rhos, gammas, inScales, dict_model, \\\n",
    "           model_counter, error_per_N, error_over_N, best_N_model\n",
    "    \n",
    "    # Avoid copying from CPU to GPU    \n",
    "    # Init container variables directly on GPU\n",
    "    ut = torch.zeros((K,1),dtype=dtype, device=torch.device(DEVICE)) \n",
    "    tp = torch.zeros((K,1),dtype=dtype, device=torch.device(DEVICE))\n",
    "    \n",
    "    # Init matrices directly on GPU\n",
    "    Wres = get_matrixGPU(N,rr,sr)\n",
    "    Win, Wfb, Wout = get_weight_matricesGPU(K,N,L,ri,RF)\n",
    "    Wnet = torch.cat((Win,Wres,Wfb),1) # Concat to one matrix for faster compute\n",
    "    xt, yt = set_vectorsGPU(N,L,rr) # On GPU random init x0 and container yt   \n",
    "    \n",
    "    # GPU containers for Phi and y in regression solve (bad naming... reused var name)\n",
    "    # Here states is the Phi matrix\n",
    "    states = torch.zeros((TRAINLENGTH, N+K),dtype=dtype,\n",
    "                         device=DEVICE)\n",
    "    targets = torch.zeros((TRAINLENGTH),dtype=dtype,\n",
    "                        device=DEVICE)\n",
    "    # Loop through training data and accumulate states for ridge-regression solve\n",
    "    for i in range(TRAINLENGTH):\n",
    "        ut[:,0] = s[i]                         # Forcing u[t] \n",
    "        tp[:,0] = s[i+1]                       # True target for prediction u[t+1] \n",
    "        uxy = torch.cat((ut,xt,yt),0)          # Concat vectors for use with Wnet\n",
    "        xt1 = update_res_stateGPU(Wnet,xt,uxy,a,amp) # x[t+1] = F(x[t],u[t])\n",
    "        xu = torch.transpose(torch.cat((xt1,ut),0),0,1).to(DEVICE) # Transpose as row for Phi\n",
    "        states[i,:] = xu[0,:]\n",
    "        xt, yt = xt1, tp \n",
    "\n",
    "    state = states.detach().cpu().numpy() \n",
    "    #target = targets.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    ############################             Ridge Regression solve on CPU (fast!)\n",
    "    torch.cuda.synchronize()    # GPU threads were running asynchronous\n",
    "                                # Use signal since already sitting on CPU side\n",
    "    wout = rct.get_trained_weights(state[BURNIN:],\n",
    "                                   signal[BURNIN+1:TRAINLENGTH+1],\n",
    "                                   REG)\n",
    "                                # Move back to GPU\n",
    "    Wout = torch.from_numpy(wout.reshape(L,N+K)).type(dtype).cuda(DEVICE) # Trained Wout\n",
    "    torch.cuda.synchronize()    # Make sure synchronized before prediction pass\n",
    "    ############################\n",
    "    \n",
    "    # Container for all predictions... Burnin --> train --> test --> freerun\n",
    "    predictions = torch.zeros((M+int(FREERUN/deltaT),K),\n",
    "                              dtype=dtype,\n",
    "                              device=torch.device(DEVICE))\n",
    "    # Reset new initial vectors for prediction pass\n",
    "    xt, yt = set_vectorsGPU(N,L,rr)\n",
    "    ut.fill_(0.0)\n",
    "    for i in range(M+int(FREERUN/deltaT)):\n",
    "        if(i < TRAINLENGTH):\n",
    "            ut[:,0] = s[i]\n",
    "        else:\n",
    "            #pdb.set_trace()\n",
    "            ut = yt\n",
    "        uxy = torch.cat((ut,xt,yt),0)\n",
    "        xt1 = update_res_stateGPU(Wnet,xt,uxy,a,amp)\n",
    "        xu = torch.cat((xt1,ut),0)\n",
    "        yt1 = predict_yGPU(Wout,xu)\n",
    "        #pdb.set_trace()\n",
    "        predictions[i] = yt1[:,0]\n",
    "        xt, yt = xt1, yt1\n",
    "\n",
    "    yHat_GPU = predictions.detach().cpu().numpy()  # Move predictions onto CPU (numpy)\n",
    "    \n",
    "    nrmse3D = np.ones((K,1))*1000\n",
    "    try:\n",
    "        for i in range(K):\n",
    "            nrmse3D[i,0] = rca.NRMSE(signal[TRAINLENGTH:TRAINLENGTH+TEST,i],\n",
    "                                   yHat_GPU[TRAINLENGTH:TRAINLENGTH+TEST,i],\n",
    "                                   MINMAXS[i])\n",
    "    except: \n",
    "        pass\n",
    "    #### Pearson Correlation <=> Cosine Distance    \n",
    "    av = signal[TRAINLENGTH:TRAINLENGTH+TEST]\n",
    "    bv = np.squeeze(yHat_GPU[TRAINLENGTH:TRAINLENGTH+TEST])\n",
    "    dists = np.zeros(K)\n",
    "    for i in range(K):\n",
    "        avec = av[:,i].reshape(TEST,1)\n",
    "        bvec = bv[:,i].reshape(TEST,1)\n",
    "        num = np.squeeze(np.dot(avec.T,bvec))\n",
    "        den = np.linalg.norm(avec)*np.linalg.norm(bvec)\n",
    "        cosine_similarity = num/den\n",
    "        cosine_distance = 1 - cosine_similarity\n",
    "        dists[i] = cosine_distance\n",
    "    dist = np.max(dists)\n",
    "    nrmse = np.max(nrmse3D)\n",
    "    loss = np.max(nrmse3D+dists)\n",
    "    if(np.isnan(loss) or (np.isinf(loss) or (loss > 1000.0))):\n",
    "        loss = 1000\n",
    "    \n",
    "    if((loss < running_error) and (nrmse < .17)):\n",
    "        print(f'Dist {dist:3f}')\n",
    "        running_error = loss\n",
    "        wnet = Wnet.detach().cpu().numpy()\n",
    "        currentParams = np.array([a,sr,amp,ri,rr,loss])\n",
    "        if(error_over_N > running_error):\n",
    "            print('\\n\\nNew N best!!!!!!!!!!!!\\n\\n')\n",
    "            error_over_N = running_error  #set the lowest error\n",
    "            best_N_model = [N,ref,rn,counter]\n",
    "            states_dict = {'States': state}\n",
    "            with open(f'Dicts/States/states_L3D_{N}.json', 'w') as fp:\n",
    "                json.dump(states_dict, fp, cls=NumpyArrayEncoder)\n",
    "            dict_model[str(N)] = {'Wnet': wnet,\n",
    "                                  'Wout': wout,\n",
    "                                  'Preds': yHat_GPU,\n",
    "                                  'Params': currentParams}\n",
    "            print(best_N_model)\n",
    "        \n",
    "        alphas.append(a)\n",
    "        rhos.append(sr)\n",
    "        gammas.append(amp)\n",
    "        inScales.append(ri)\n",
    "        resScales.append(rr)\n",
    "                \n",
    "        fig = plt.figure(figsize=(15,12))\n",
    "        plt.subplot(3,1,1)\n",
    "        plt.plot(signal[TRAINLENGTH-LEAD:,0], label='Target')\n",
    "        plt.plot(yHat_GPU[TRAINLENGTH-LEAD:,0], label='X pred')\n",
    "        plt.axvline(LEAD,c='r',linestyle='dashed')\n",
    "        plt.ylim(-20,20)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(3,1,2)\n",
    "        plt.plot(signal[TRAINLENGTH-LEAD:,1], label='Target')\n",
    "        plt.plot(yHat_GPU[TRAINLENGTH-LEAD:,1], label='Y pred')\n",
    "        plt.axvline(LEAD,c='r',linestyle='dashed')\n",
    "        plt.ylim(-20,20)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(3,1,3)\n",
    "        plt.plot(signal[TRAINLENGTH-LEAD:,2], label='Target')\n",
    "        plt.plot(yHat_GPU[TRAINLENGTH-LEAD:,2], label='Z pred')\n",
    "        plt.axvline(LEAD,c='r',linestyle='dashed')\n",
    "        plt.ylim(-20,20)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        model_counter += 1\n",
    "        error_per_N.append(nrmse3D)\n",
    "\n",
    "        print(f' Iter={counter} a={a:.3f} sr={sr:.3f} amp={amp:.3f}',\n",
    "              f' ri={ri:.3f} rr={rr:.3f} loss={loss:3f}\\n\\n')\n",
    "    print(f'Iter: {counter} #### Diagnostic {loss:3f}   Time {(time.time()-start):.2f}',\n",
    "          f' Best {running_error:.3f} NRMSE {np.max(nrmse):.3f} CD {np.max(dists):.3f}')\n",
    "    counter += 1\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Search with gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Size28 ref 0 -- Run 0 ***********\n",
      "Iter: 0 #### Diagnostic 1.486196   Time 7.24  Best 1000.000 NRMSE 0.325 CD 1.161\n",
      "Iter: 1 #### Diagnostic 1000.000000   Time 4.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 2 #### Diagnostic 1000.000000   Time 3.83  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 3 #### Diagnostic 0.970743   Time 3.88  Best 1000.000 NRMSE 0.232 CD 0.738\n",
      "Iter: 4 #### Diagnostic 1.822358   Time 3.92  Best 1000.000 NRMSE 0.780 CD 1.042\n",
      "Iter: 5 #### Diagnostic 0.988004   Time 3.77  Best 1000.000 NRMSE 0.251 CD 0.737\n",
      "Iter: 6 #### Diagnostic 1000.000000   Time 3.90  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 7 #### Diagnostic 5.971205   Time 3.83  Best 1000.000 NRMSE 4.983 CD 0.989\n",
      "Iter: 8 #### Diagnostic 1.548294   Time 3.88  Best 1000.000 NRMSE 0.493 CD 1.055\n",
      "Iter: 9 #### Diagnostic 1000.000000   Time 3.77  Best 1000.000 NRMSE 330951552565047001088.000 CD 1.000\n",
      "Iter: 10 #### Diagnostic 1000.000000   Time 3.92  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 11 #### Diagnostic 1000.000000   Time 3.68  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 12 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 13 #### Diagnostic 1.297127   Time 3.65  Best 1000.000 NRMSE 0.314 CD 0.983\n",
      "Iter: 14 #### Diagnostic 1.359051   Time 3.65  Best 1000.000 NRMSE 0.296 CD 1.063\n",
      "Iter: 15 #### Diagnostic 1000.000000   Time 3.66  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 16 #### Diagnostic 1000.000000   Time 3.62  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 17 #### Diagnostic 1.767663   Time 3.67  Best 1000.000 NRMSE 0.678 CD 1.089\n",
      "Iter: 18 #### Diagnostic 1.854302   Time 3.69  Best 1000.000 NRMSE 0.850 CD 1.004\n",
      "Iter: 19 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 20 #### Diagnostic 1000.000000   Time 3.66  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 21 #### Diagnostic 1.477482   Time 3.67  Best 1000.000 NRMSE 0.498 CD 0.980\n",
      "Iter: 22 #### Diagnostic 1.124085   Time 3.66  Best 1000.000 NRMSE 0.274 CD 0.851\n",
      "Iter: 23 #### Diagnostic 1000.000000   Time 3.66  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 24 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 25 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 26 #### Diagnostic 1.185044   Time 3.65  Best 1000.000 NRMSE 0.272 CD 0.913\n",
      "Iter: 27 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 28 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 29 #### Diagnostic 1.252458   Time 3.66  Best 1000.000 NRMSE 0.287 CD 0.966\n",
      "Iter: 30 #### Diagnostic 1.413480   Time 3.60  Best 1000.000 NRMSE 0.410 CD 1.004\n",
      "Iter: 31 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 32 #### Diagnostic 1000.000000   Time 3.71  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 33 #### Diagnostic 1.346168   Time 3.72  Best 1000.000 NRMSE 0.303 CD 1.044\n",
      "Iter: 34 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 35 #### Diagnostic 2.068259   Time 3.65  Best 1000.000 NRMSE 1.028 CD 1.041\n",
      "Iter: 36 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 37 #### Diagnostic 1000.000000   Time 3.66  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 38 #### Diagnostic 1.127066   Time 3.75  Best 1000.000 NRMSE 0.270 CD 0.857\n",
      "Iter: 39 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 40 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 41 #### Diagnostic 1.171271   Time 3.69  Best 1000.000 NRMSE 0.258 CD 0.914\n",
      "Iter: 42 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 43 #### Diagnostic 1.563090   Time 3.66  Best 1000.000 NRMSE 0.300 CD 1.263\n",
      "Iter: 44 #### Diagnostic 1.299686   Time 3.66  Best 1000.000 NRMSE 0.288 CD 1.011\n",
      "Iter: 45 #### Diagnostic 1.635339   Time 3.62  Best 1000.000 NRMSE 0.554 CD 1.081\n",
      "Iter: 46 #### Diagnostic 1000.000000   Time 3.63  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 47 #### Diagnostic 1.294161   Time 3.63  Best 1000.000 NRMSE 0.303 CD 0.991\n",
      "Iter: 48 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 49 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 50 #### Diagnostic 1000.000000   Time 3.67  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 51 #### Diagnostic 1.139938   Time 3.64  Best 1000.000 NRMSE 0.262 CD 0.878\n",
      "Iter: 52 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 53 #### Diagnostic 1000.000000   Time 3.69  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 54 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 55 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 56 #### Diagnostic 1000.000000   Time 3.66  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 57 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 58 #### Diagnostic 1.369164   Time 3.64  Best 1000.000 NRMSE 0.306 CD 1.063\n",
      "Iter: 59 #### Diagnostic 1.069377   Time 3.66  Best 1000.000 NRMSE 0.259 CD 0.810\n",
      "Iter: 60 #### Diagnostic 1.428558   Time 3.59  Best 1000.000 NRMSE 0.335 CD 1.094\n",
      "Iter: 61 #### Diagnostic 1000.000000   Time 3.63  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 62 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 63 #### Diagnostic 1000.000000   Time 3.63  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 64 #### Diagnostic 1.550512   Time 3.66  Best 1000.000 NRMSE 0.288 CD 1.263\n",
      "Iter: 65 #### Diagnostic 1.594101   Time 3.64  Best 1000.000 NRMSE 0.526 CD 1.068\n",
      "Iter: 66 #### Diagnostic 1.251165   Time 3.68  Best 1000.000 NRMSE 0.282 CD 0.969\n",
      "Iter: 67 #### Diagnostic 1.209550   Time 3.68  Best 1000.000 NRMSE 0.279 CD 0.930\n",
      "Iter: 68 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 69 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 2147150032624321536.000 CD 1.000\n",
      "Iter: 70 #### Diagnostic 1000.000000   Time 3.66  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 71 #### Diagnostic 0.998586   Time 3.66  Best 1000.000 NRMSE 0.250 CD 0.749\n",
      "Iter: 72 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 73 #### Diagnostic 1.225492   Time 3.66  Best 1000.000 NRMSE 0.281 CD 0.944\n",
      "Iter: 74 #### Diagnostic 1000.000000   Time 3.60  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 75 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 76 #### Diagnostic 1.376121   Time 3.74  Best 1000.000 NRMSE 0.324 CD 1.053\n",
      "Iter: 77 #### Diagnostic 1000.000000   Time 4.31  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 78 #### Diagnostic 1000.000000   Time 5.12  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 79 #### Diagnostic 1000.000000   Time 5.01  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 80 #### Diagnostic 1000.000000   Time 5.02  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 81 #### Diagnostic 1000.000000   Time 4.87  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 82 #### Diagnostic 1000.000000   Time 4.29  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 83 #### Diagnostic 1.404925   Time 4.97  Best 1000.000 NRMSE 0.322 CD 1.083\n",
      "Iter: 84 #### Diagnostic 1000.000000   Time 3.92  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 85 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 86 #### Diagnostic 1.392208   Time 3.64  Best 1000.000 NRMSE 0.297 CD 1.095\n",
      "Iter: 87 #### Diagnostic 3.072240   Time 3.64  Best 1000.000 NRMSE 1.984 CD 1.088\n",
      "Iter: 88 #### Diagnostic 1000.000000   Time 3.63  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 89 #### Diagnostic 1.418536   Time 3.64  Best 1000.000 NRMSE 0.459 CD 0.960\n",
      "Iter: 90 #### Diagnostic 1.517094   Time 3.66  Best 1000.000 NRMSE 0.293 CD 1.224\n",
      "Iter: 91 #### Diagnostic 1.364763   Time 3.73  Best 1000.000 NRMSE 0.286 CD 1.079\n",
      "Iter: 92 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 185607719220131934335043581192962048.000 CD nan\n",
      "Iter: 93 #### Diagnostic 1.435671   Time 3.64  Best 1000.000 NRMSE 0.309 CD 1.126\n",
      "Iter: 94 #### Diagnostic 1.526332   Time 3.63  Best 1000.000 NRMSE 0.317 CD 1.210\n",
      "Iter: 95 #### Diagnostic 2.260124   Time 3.64  Best 1000.000 NRMSE 1.263 CD 0.998\n",
      "Iter: 96 #### Diagnostic 1000.000000   Time 3.67  Best 1000.000 NRMSE 1000.000 CD nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 97 #### Diagnostic 1000.000000   Time 3.64  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 98 #### Diagnostic 1000.000000   Time 3.67  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 99 #### Diagnostic 1.264789   Time 3.64  Best 1000.000 NRMSE 0.288 CD 0.977\n",
      "End Refinement Run 0 Time 382.957\n",
      "\n",
      "Run: 0 Best result = 0.9707433696159371\n",
      "a = 0.8141554887806006\n",
      "sr = 1.4525995097983775\n",
      "amp = 0.0763152593144034\n",
      "ri = 2.094143937810471\n",
      "********** Size28 ref 0 -- Run 1 ***********\n",
      "Iter: 0 #### Diagnostic 1.389352   Time 3.67  Best 1000.000 NRMSE 0.323 CD 1.067\n",
      "Iter: 1 #### Diagnostic 4.390088   Time 3.67  Best 1000.000 NRMSE 3.383 CD 1.007\n",
      "Iter: 2 #### Diagnostic 1000.000000   Time 3.67  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 3 #### Diagnostic 1.246069   Time 3.73  Best 1000.000 NRMSE 0.339 CD 0.907\n",
      "Iter: 4 #### Diagnostic 1.355951   Time 3.65  Best 1000.000 NRMSE 0.279 CD 1.077\n",
      "Iter: 5 #### Diagnostic 0.994351   Time 3.67  Best 1000.000 NRMSE 0.249 CD 0.745\n",
      "Iter: 6 #### Diagnostic 1.163298   Time 3.65  Best 1000.000 NRMSE 0.278 CD 0.886\n",
      "Iter: 7 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 8 #### Diagnostic 1000.000000   Time 3.66  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 9 #### Diagnostic 1.670991   Time 3.63  Best 1000.000 NRMSE 0.572 CD 1.099\n",
      "Iter: 10 #### Diagnostic 1.478083   Time 3.64  Best 1000.000 NRMSE 0.440 CD 1.038\n",
      "Iter: 11 #### Diagnostic 5.316697   Time 3.71  Best 1000.000 NRMSE 4.324 CD 0.993\n",
      "Iter: 12 #### Diagnostic 1000.000000   Time 3.69  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 13 #### Diagnostic 1.438468   Time 3.68  Best 1000.000 NRMSE 0.350 CD 1.088\n",
      "Iter: 14 #### Diagnostic 1000.000000   Time 3.68  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 15 #### Diagnostic 1.213783   Time 3.83  Best 1000.000 NRMSE 0.284 CD 0.930\n",
      "Iter: 16 #### Diagnostic 1.205746   Time 4.99  Best 1000.000 NRMSE 0.274 CD 0.932\n",
      "Iter: 17 #### Diagnostic 1000.000000   Time 5.41  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 18 #### Diagnostic 1000.000000   Time 5.42  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 19 #### Diagnostic 1000.000000   Time 5.46  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 20 #### Diagnostic 1.377551   Time 4.96  Best 1000.000 NRMSE 0.309 CD 1.068\n",
      "Iter: 21 #### Diagnostic 1.358886   Time 3.76  Best 1000.000 NRMSE 0.301 CD 1.058\n",
      "Iter: 22 #### Diagnostic 1000.000000   Time 3.92  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 23 #### Diagnostic 1000.000000   Time 3.96  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 24 #### Diagnostic 3.216843   Time 3.67  Best 1000.000 NRMSE 2.167 CD 1.050\n",
      "Iter: 25 #### Diagnostic 1000.000000   Time 3.99  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 26 #### Diagnostic 1000.000000   Time 3.95  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 27 #### Diagnostic 1.292402   Time 3.75  Best 1000.000 NRMSE 0.273 CD 1.020\n",
      "Iter: 28 #### Diagnostic 1000.000000   Time 3.91  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 29 #### Diagnostic 1.487198   Time 3.72  Best 1000.000 NRMSE 0.288 CD 1.200\n",
      "Iter: 30 #### Diagnostic 2.168109   Time 3.88  Best 1000.000 NRMSE 1.167 CD 1.001\n",
      "Iter: 31 #### Diagnostic 1.270370   Time 3.96  Best 1000.000 NRMSE 0.287 CD 0.984\n",
      "Iter: 32 #### Diagnostic 1000.000000   Time 3.78  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 33 #### Diagnostic 1000.000000   Time 3.97  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 34 #### Diagnostic 1.432366   Time 3.74  Best 1000.000 NRMSE 0.430 CD 1.003\n",
      "Iter: 35 #### Diagnostic 1000.000000   Time 3.87  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 36 #### Diagnostic 1.139033   Time 4.13  Best 1000.000 NRMSE 0.269 CD 0.870\n",
      "Iter: 37 #### Diagnostic 1000.000000   Time 3.71  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 38 #### Diagnostic 1000.000000   Time 3.92  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 39 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 40 #### Diagnostic 0.925054   Time 3.89  Best 1000.000 NRMSE 0.240 CD 0.685\n",
      "Iter: 41 #### Diagnostic 1000.000000   Time 3.93  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 42 #### Diagnostic 1000.000000   Time 3.68  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 43 #### Diagnostic 1000.000000   Time 3.95  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 44 #### Diagnostic 1000.000000   Time 3.79  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 45 #### Diagnostic 1.264415   Time 3.82  Best 1000.000 NRMSE 0.343 CD 0.921\n",
      "Iter: 46 #### Diagnostic 1000.000000   Time 3.92  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 47 #### Diagnostic 1.338198   Time 3.65  Best 1000.000 NRMSE 0.281 CD 1.057\n",
      "Iter: 48 #### Diagnostic 1000.000000   Time 3.95  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 49 #### Diagnostic 1000.000000   Time 3.94  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 50 #### Diagnostic 1000.000000   Time 3.84  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 51 #### Diagnostic 1000.000000   Time 4.22  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 52 #### Diagnostic 1000.000000   Time 3.73  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 53 #### Diagnostic 3.116758   Time 4.01  Best 1000.000 NRMSE 2.064 CD 1.053\n",
      "Iter: 54 #### Diagnostic 1000.000000   Time 3.65  Best 1000.000 NRMSE 6296134170496937408677806080.000 CD 1.000\n",
      "Iter: 55 #### Diagnostic 1000.000000   Time 3.99  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 56 #### Diagnostic 1.088624   Time 3.71  Best 1000.000 NRMSE 0.250 CD 0.839\n",
      "Iter: 57 #### Diagnostic 1000.000000   Time 4.04  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 58 #### Diagnostic 1.050370   Time 3.93  Best 1000.000 NRMSE 0.244 CD 0.806\n",
      "Iter: 59 #### Diagnostic 1.053135   Time 3.84  Best 1000.000 NRMSE 0.258 CD 0.795\n",
      "Iter: 60 #### Diagnostic 1000.000000   Time 4.02  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 61 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 62 #### Diagnostic 1000.000000   Time 4.03  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 63 #### Diagnostic 1.397231   Time 3.70  Best 1000.000 NRMSE 0.297 CD 1.100\n",
      "Iter: 64 #### Diagnostic 1.385693   Time 4.15  Best 1000.000 NRMSE 0.298 CD 1.088\n",
      "Iter: 65 #### Diagnostic 1000.000000   Time 3.78  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 66 #### Diagnostic 1000.000000   Time 4.01  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 67 #### Diagnostic 1.910907   Time 3.98  Best 1000.000 NRMSE 0.874 CD 1.037\n",
      "Iter: 68 #### Diagnostic 1.234238   Time 3.85  Best 1000.000 NRMSE 0.280 CD 0.954\n",
      "Iter: 69 #### Diagnostic 1.099268   Time 3.98  Best 1000.000 NRMSE 0.264 CD 0.835\n",
      "Iter: 70 #### Diagnostic 2.565567   Time 3.84  Best 1000.000 NRMSE 1.583 CD 0.983\n",
      "Iter: 71 #### Diagnostic 1.460259   Time 4.04  Best 1000.000 NRMSE 0.312 CD 1.148\n",
      "Iter: 72 #### Diagnostic 1000.000000   Time 3.83  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 73 #### Diagnostic 1.405988   Time 4.07  Best 1000.000 NRMSE 0.340 CD 1.066\n",
      "Iter: 74 #### Diagnostic 1000.000000   Time 3.73  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 75 #### Diagnostic 1.933969   Time 4.04  Best 1000.000 NRMSE 0.946 CD 0.988\n",
      "Iter: 76 #### Diagnostic 1000.000000   Time 3.81  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 77 #### Diagnostic 1.384259   Time 4.00  Best 1000.000 NRMSE 0.284 CD 1.100\n",
      "Iter: 78 #### Diagnostic 2.424602   Time 3.87  Best 1000.000 NRMSE 1.337 CD 1.088\n",
      "Iter: 79 #### Diagnostic 1.405948   Time 3.84  Best 1000.000 NRMSE 0.306 CD 1.099\n",
      "Iter: 80 #### Diagnostic 2.358615   Time 4.11  Best 1000.000 NRMSE 1.407 CD 0.951\n",
      "Iter: 81 #### Diagnostic 1000.000000   Time 3.83  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 82 #### Diagnostic 1.480075   Time 3.93  Best 1000.000 NRMSE 0.495 CD 0.985\n",
      "Iter: 83 #### Diagnostic 1000.000000   Time 4.13  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 84 #### Diagnostic 1.299904   Time 4.30  Best 1000.000 NRMSE 0.293 CD 1.007\n",
      "Iter: 85 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 86 #### Diagnostic 1000.000000   Time 4.19  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 87 #### Diagnostic 1000.000000   Time 3.86  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 88 #### Diagnostic 0.947514   Time 4.13  Best 1000.000 NRMSE 0.245 CD 0.703\n",
      "Iter: 89 #### Diagnostic 1.201675   Time 3.81  Best 1000.000 NRMSE 0.276 CD 0.926\n",
      "Iter: 90 #### Diagnostic 1.615998   Time 4.16  Best 1000.000 NRMSE 0.611 CD 1.005\n",
      "Iter: 91 #### Diagnostic 1.766658   Time 4.12  Best 1000.000 NRMSE 0.771 CD 0.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 92 #### Diagnostic 1.585025   Time 4.27  Best 1000.000 NRMSE 0.402 CD 1.183\n",
      "Iter: 93 #### Diagnostic 1000.000000   Time 3.86  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 94 #### Diagnostic 1000.000000   Time 4.17  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 95 #### Diagnostic 1000.000000   Time 3.75  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 96 #### Diagnostic 1.250735   Time 4.33  Best 1000.000 NRMSE 0.289 CD 0.962\n",
      "Iter: 97 #### Diagnostic 1.344414   Time 3.80  Best 1000.000 NRMSE 0.390 CD 0.955\n",
      "Iter: 98 #### Diagnostic 1.473607   Time 4.08  Best 1000.000 NRMSE 0.280 CD 1.193\n",
      "Iter: 99 #### Diagnostic 1.646611   Time 3.84  Best 1000.000 NRMSE 0.638 CD 1.009\n",
      "End Refinement Run 0 Time 397.611\n",
      "\n",
      "Run: 1 Best result = 0.9250542264167424\n",
      "a = 0.5326157130249747\n",
      "sr = 1.2829820541260208\n",
      "amp = 0.01002988500756875\n",
      "ri = 2.620123636472476\n",
      "********** Size28 ref 0 -- Run 2 ***********\n",
      "Iter: 0 #### Diagnostic 1.164898   Time 4.35  Best 1000.000 NRMSE 0.273 CD 0.891\n",
      "Iter: 1 #### Diagnostic 1.471653   Time 3.84  Best 1000.000 NRMSE 0.312 CD 1.160\n",
      "Iter: 2 #### Diagnostic 1.319447   Time 4.34  Best 1000.000 NRMSE 0.302 CD 1.018\n",
      "Iter: 3 #### Diagnostic 1000.000000   Time 3.77  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 4 #### Diagnostic 1.221605   Time 3.76  Best 1000.000 NRMSE 0.282 CD 0.939\n",
      "Iter: 5 #### Diagnostic 1000.000000   Time 3.77  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 6 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 7 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 8 #### Diagnostic 1000.000000   Time 3.71  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 9 #### Diagnostic 1.292454   Time 3.78  Best 1000.000 NRMSE 0.294 CD 0.998\n",
      "Iter: 10 #### Diagnostic 1.247998   Time 4.04  Best 1000.000 NRMSE 0.272 CD 0.976\n",
      "Iter: 11 #### Diagnostic 1000.000000   Time 3.69  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 12 #### Diagnostic 1000.000000   Time 3.72  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 13 #### Diagnostic 2.537192   Time 3.76  Best 1000.000 NRMSE 1.579 CD 0.959\n",
      "Iter: 14 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 15 #### Diagnostic 1000.000000   Time 3.75  Best 1000.000 NRMSE 106673694788103684096.000 CD 1.000\n",
      "Iter: 16 #### Diagnostic 1.387884   Time 3.75  Best 1000.000 NRMSE 0.322 CD 1.066\n",
      "Iter: 17 #### Diagnostic 1000.000000   Time 3.83  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 18 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 19 #### Diagnostic 1000.000000   Time 3.72  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 20 #### Diagnostic 1000.000000   Time 3.71  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 21 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 22 #### Diagnostic 1.302670   Time 3.73  Best 1000.000 NRMSE 0.284 CD 1.019\n",
      "Iter: 23 #### Diagnostic 1000.000000   Time 3.72  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 24 #### Diagnostic 1000.000000   Time 3.72  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 25 #### Diagnostic 14.861074   Time 3.71  Best 1000.000 NRMSE 13.755 CD 1.106\n",
      "Iter: 26 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 27 #### Diagnostic 1000.000000   Time 3.72  Best 1000.000 NRMSE 5411429306498046976.000 CD 1.000\n",
      "Iter: 28 #### Diagnostic 1000.000000   Time 3.71  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 29 #### Diagnostic 1.181157   Time 3.72  Best 1000.000 NRMSE 0.279 CD 0.903\n",
      "Iter: 30 #### Diagnostic 1000.000000   Time 3.73  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 31 #### Diagnostic 0.726097   Time 3.74  Best 1000.000 NRMSE 0.201 CD 0.525\n",
      "Iter: 32 #### Diagnostic 1000.000000   Time 3.71  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 33 #### Diagnostic 1.238469   Time 3.74  Best 1000.000 NRMSE 0.276 CD 0.963\n",
      "Iter: 34 #### Diagnostic 0.987454   Time 3.73  Best 1000.000 NRMSE 0.232 CD 0.755\n",
      "Iter: 35 #### Diagnostic 1000.000000   Time 3.72  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 36 #### Diagnostic 1.183789   Time 3.72  Best 1000.000 NRMSE 0.274 CD 0.910\n",
      "Iter: 37 #### Diagnostic 1000.000000   Time 3.68  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 38 #### Diagnostic 1000.000000   Time 3.71  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 39 #### Diagnostic 1.456796   Time 3.72  Best 1000.000 NRMSE 0.299 CD 1.157\n",
      "Iter: 40 #### Diagnostic 1000.000000   Time 3.73  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 41 #### Diagnostic 1000.000000   Time 3.86  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 42 #### Diagnostic 1.337948   Time 3.69  Best 1000.000 NRMSE 0.290 CD 1.048\n",
      "Iter: 43 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 44 #### Diagnostic 1000.000000   Time 3.77  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 45 #### Diagnostic 1.339179   Time 3.71  Best 1000.000 NRMSE 0.294 CD 1.045\n",
      "Iter: 46 #### Diagnostic 1000.000000   Time 3.79  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 47 #### Diagnostic 1.160738   Time 3.72  Best 1000.000 NRMSE 0.275 CD 0.886\n",
      "Iter: 48 #### Diagnostic 3.330555   Time 3.73  Best 1000.000 NRMSE 2.282 CD 1.049\n",
      "Iter: 49 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 50 #### Diagnostic 1.380187   Time 3.79  Best 1000.000 NRMSE 0.313 CD 1.067\n",
      "Iter: 51 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 52 #### Diagnostic 1.391572   Time 3.73  Best 1000.000 NRMSE 0.296 CD 1.096\n",
      "Iter: 53 #### Diagnostic 1.174296   Time 3.74  Best 1000.000 NRMSE 0.262 CD 0.912\n",
      "Iter: 54 #### Diagnostic 1000.000000   Time 3.66  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 55 #### Diagnostic 1000.000000   Time 3.77  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 56 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 57 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 58 #### Diagnostic 1.496828   Time 3.72  Best 1000.000 NRMSE 0.323 CD 1.174\n",
      "Iter: 59 #### Diagnostic 1.225433   Time 3.75  Best 1000.000 NRMSE 0.258 CD 0.968\n",
      "Iter: 60 #### Diagnostic 1000.000000   Time 4.07  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 61 #### Diagnostic 1000.000000   Time 3.97  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 62 #### Diagnostic 1000.000000   Time 3.84  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 63 #### Diagnostic 3.710158   Time 3.73  Best 1000.000 NRMSE 2.710 CD 1.000\n",
      "Iter: 64 #### Diagnostic 1.439949   Time 3.73  Best 1000.000 NRMSE 0.341 CD 1.099\n",
      "Iter: 65 #### Diagnostic 1.385233   Time 3.76  Best 1000.000 NRMSE 0.304 CD 1.082\n",
      "Iter: 66 #### Diagnostic 1000.000000   Time 3.73  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 67 #### Diagnostic 1.280050   Time 3.73  Best 1000.000 NRMSE 0.298 CD 0.983\n",
      "Iter: 68 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 69 #### Diagnostic 2.224880   Time 3.73  Best 1000.000 NRMSE 1.222 CD 1.003\n",
      "Iter: 70 #### Diagnostic 1000.000000   Time 3.93  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 71 #### Diagnostic 1000.000000   Time 4.18  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 72 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 73 #### Diagnostic 1.336605   Time 3.68  Best 1000.000 NRMSE 0.268 CD 1.069\n",
      "Iter: 74 #### Diagnostic 2.989830   Time 3.73  Best 1000.000 NRMSE 1.988 CD 1.002\n",
      "Iter: 75 #### Diagnostic 1000.000000   Time 3.72  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 76 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 77 #### Diagnostic 1000.000000   Time 3.77  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 78 #### Diagnostic 1.729008   Time 3.75  Best 1000.000 NRMSE 0.543 CD 1.186\n",
      "Iter: 79 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 80 #### Diagnostic 1000.000000   Time 3.82  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 81 #### Diagnostic 1.161368   Time 3.97  Best 1000.000 NRMSE 0.243 CD 0.918\n",
      "Iter: 82 #### Diagnostic 1000.000000   Time 3.80  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 83 #### Diagnostic 1000.000000   Time 3.88  Best 1000.000 NRMSE 25748.140 CD 1.028\n",
      "Iter: 84 #### Diagnostic 1.320959   Time 3.70  Best 1000.000 NRMSE 0.293 CD 1.028\n",
      "Iter: 85 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 86 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 87 #### Diagnostic 1.359714   Time 3.75  Best 1000.000 NRMSE 0.295 CD 1.064\n",
      "Iter: 88 #### Diagnostic 1000.000000   Time 3.75  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 89 #### Diagnostic 1000.000000   Time 3.71  Best 1000.000 NRMSE 703032453575536541696.000 CD 1.000\n",
      "Iter: 90 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 91 #### Diagnostic 3.362945   Time 3.75  Best 1000.000 NRMSE 2.317 CD 1.046\n",
      "Iter: 92 #### Diagnostic 1000.000000   Time 3.76  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 93 #### Diagnostic 1.648742   Time 3.75  Best 1000.000 NRMSE 0.638 CD 1.011\n",
      "Iter: 94 #### Diagnostic 3.431262   Time 3.74  Best 1000.000 NRMSE 2.384 CD 1.047\n",
      "Iter: 95 #### Diagnostic 1000.000000   Time 3.77  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 96 #### Diagnostic 1000.000000   Time 3.85  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 97 #### Diagnostic 1.860341   Time 3.75  Best 1000.000 NRMSE 0.789 CD 1.071\n",
      "Iter: 98 #### Diagnostic 1000.000000   Time 3.72  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 99 #### Diagnostic 13.131737   Time 3.77  Best 1000.000 NRMSE 12.132 CD 1.000\n",
      "End Refinement Run 0 Time 380.085\n",
      "\n",
      "Run: 2 Best result = 0.7260969721053487\n",
      "a = 0.5585264881021473\n",
      "sr = 1.6972659079273769\n",
      "amp = 0.042015618228888885\n",
      "ri = 1.8621918859313813\n",
      "********** Size28 ref 0 -- Run 3 ***********\n",
      "Iter: 0 #### Diagnostic 1000.000000   Time 3.70  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 1 #### Diagnostic 1000.000000   Time 3.82  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 2 #### Diagnostic 2.538557   Time 3.73  Best 1000.000 NRMSE 1.418 CD 1.121\n",
      "Iter: 3 #### Diagnostic 1000.000000   Time 3.74  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 4 #### Diagnostic 1000.000000   Time 3.68  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 5 #### Diagnostic 1.212870   Time 3.72  Best 1000.000 NRMSE 0.302 CD 0.911\n",
      "Iter: 6 #### Diagnostic 1000.000000   Time 3.78  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 7 #### Diagnostic 1000.000000   Time 3.93  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 8 #### Diagnostic 1.488978   Time 3.85  Best 1000.000 NRMSE 0.380 CD 1.109\n",
      "Iter: 9 #### Diagnostic 1000.000000   Time 4.11  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 10 #### Diagnostic 1000.000000   Time 4.08  Best 1000.000 NRMSE 1000.000 CD nan\n",
      "Iter: 11 #### Diagnostic 1.452712   Time 3.93  Best 1000.000 NRMSE 0.311 CD 1.142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CALLS = 100      \n",
    "s = torch.torch.from_numpy(signal).cuda(DEVICE).type(dtype)\n",
    "# Skipped N=1000,900,800\n",
    "#size = [300,50,40,30,20,10]\n",
    "size = [28,26,24,22,20,18,16,14,12]\n",
    "rand_state = [11,37,3,24,91]\n",
    "\n",
    "dict_counters = {}\n",
    "dict_model = {}\n",
    "for N in size:\n",
    "    model_counter = 0\n",
    "    error_per_N =  []\n",
    "    best_N_model = [N,0,0,0] \n",
    "    \n",
    "    min_a, max_a = .5, .9\n",
    "    min_sr, max_sr = 1.1, 1.7\n",
    "    min_g, max_g = .2, .24\n",
    "    min_ri, max_ri = .01, .1\n",
    "    min_rr, max_rr = .85, 2.7\n",
    "\n",
    "    space = [Real(min_a, max_a, name='a'),\n",
    "             Real(min_sr, max_sr, name='sr'),\n",
    "             Real(min_ri, max_ri, name='ri'),\n",
    "             Real(min_rr, max_rr, name='rr')\n",
    "            ]\n",
    "    error_over_N = 1000\n",
    "    for ref in range(5):\n",
    "        \n",
    "        alphas = []\n",
    "        rhos = []\n",
    "        gammas = []\n",
    "        inScales = []\n",
    "        resScales = []\n",
    "        \n",
    "        for rn in range(5):\n",
    "            start = time.time()\n",
    "            running_error = 1000\n",
    "            counter = 0 \n",
    "            print(f'********** Size{N} ref {ref} -- Run {rn} ***********')\n",
    "            result_gp = gp_minimize(loop,\n",
    "                                    space,\n",
    "                                    n_calls=CALLS,\n",
    "                                    random_state=rand_state[rn],\n",
    "                                    n_jobs=4,\n",
    "                                    n_initial_points=100)\n",
    "            end = time.time()-start\n",
    "            print(f'End Refinement Run {ref} Time {end:.3f}')\n",
    "            print(f'\\nRun: {rn} Best result = {result_gp.fun}')\n",
    "            names = ['a','sr','amp','ri','rr']\n",
    "            for i in range(len(space)):\n",
    "                print(f'{names[i]} = {result_gp.x[i]}')\n",
    "                \n",
    "        min_a, max_a   = resize_spaces(min_a, max_a,\n",
    "                                       np.array(alphas),\n",
    "                                       isAlpha=True)\n",
    "        min_sr, max_sr = resize_spaces(min_sr, max_sr, np.array(rhos))\n",
    "        min_g, max_g   = resize_spaces(min_g, max_g, np.array(gammas))\n",
    "        min_ri, max_ri = resize_spaces(min_ri, max_ri, np.array(inScales))\n",
    "        min_rr, max_rr = resize_spaces(min_rr, max_rr, np.array(resScales))\n",
    "        print('Refined search bounds:\\n')\n",
    "        print(f'Alpha ({min_a}, {max_a})\\n')\n",
    "        print(f'Rho ({min_sr}, {max_sr})\\n')\n",
    "        print(f'Gamma ({min_g}, {max_g})\\n')\n",
    "        print(f'r-in ({min_ri}, {max_ri})\\n')\n",
    "        print(f'r-res ({min_rr}, {max_rr})\\n')\n",
    "        \n",
    "        \n",
    "    print(model_counter)\n",
    "    print(np.mean(np.array(error_per_N),axis=0))\n",
    "    if(model_counter == 0):\n",
    "        dict_counters[f'{str(N)}'] = {'numModels': model_counter,\n",
    "                                      'meanError': [np.nan,np.nan,np.nan],\n",
    "                                      'varError' : [np.nan,np.nan,np.nan]}\n",
    "    else:\n",
    "        print(np.mean(np.array(error_per_N),axis=1))\n",
    "        dict_counters[f'{str(N)}'] = {'numModels': model_counter,\n",
    "                                      'meanError': np.mean(np.array(error_per_N),axis=0).tolist(),\n",
    "                                      'varError' : np.var(np.array(error_per_N),axis=0).tolist()}\n",
    "    with open('Dicts/diag_L3D_sm.json', 'w') as fp:\n",
    "        json.dump(dict_counters, fp, cls=NumpyArrayEncoder)\n",
    "    with open('Dicts/models_L3D_sm.json', 'w') as fp:\n",
    "        json.dump(dict_model, fp, cls=NumpyArrayEncoder)\n",
    "print(dict_counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 24\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"text.usetex\"] = True \n",
    "plt.rcParams[\"axes.xmargin\"] = 0 \n",
    "plt.rcParams[\"axes.titlesize\"] = fontsize \n",
    "plt.rcParams[\"axes.labelsize\"] = fontsize\n",
    "plt.rcParams[\"xtick.labelsize\"] = fontsize\n",
    "plt.rcParams[\"ytick.labelsize\"] = fontsize\n",
    "plt.rcParams[\"axes.labelsize\"] = fontsize\n",
    "plt.rcParams[\"font.weight\"] = \"heavy\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"heavy\"\n",
    "\n",
    "fpath = 'Dicts/diag_L3D_sm.json'\n",
    "with open(fpath,'r') as j:\n",
    "    dict_diag = json.loads(j.read())\n",
    "\n",
    "fpath = 'Dicts/models_L3D_sm.json'\n",
    "with open(fpath,'r') as j:\n",
    "    dict_models = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size = [300,50,40,30,20,10]\n",
    "variable = ['X','Y','Z']\n",
    "\n",
    "fig, ax = plt.subplots(3, figsize=(10,9))\n",
    "for sp in range(3):\n",
    "    ax[sp].set_title(f'{variable[sp]} Predictions')\n",
    "    for n in size:\n",
    "        preds = np.array(dict_models[str(n)]['Preds'])\n",
    "        error_testset = rca.NRMSE(signal[TRAINLENGTH:TRAINLENGTH + TEST,sp],\n",
    "                              preds[TRAINLENGTH:TRAINLENGTH + TEST,sp],\n",
    "                              MINMAXS[sp])\n",
    "        kl,_,_,_ = rca.distribution(signal[TRAINLENGTH:TRAINLENGTH + TEST,sp],\n",
    "                                    preds[TRAINLENGTH:TRAINLENGTH + TEST,sp],\n",
    "                                     np.min(signal[:TRAINLENGTH+TEST,sp]),\n",
    "                                     np.max(signal[:TRAINLENGTH+TEST,sp]),\n",
    "                                     bins=50)\n",
    "        print(f'Dim={sp} N={n}   Error = {error_testset.round(3)} Div = {kl:.3f}')\n",
    "        ax[0].plot(signal[TRAINLENGTH:TRAINLENGTH + TEST,sp], color='k',\n",
    "                  label='target', alpha=0)\n",
    "        if(sp==2):\n",
    "            ax[sp].plot(signal[TRAINLENGTH:TRAINLENGTH + TEST,sp], color='k')\n",
    "            ax[sp].plot(preds[TRAINLENGTH:TRAINLENGTH + TEST,sp],\n",
    "                     linestyle='dotted', label=f'N={n}')\n",
    "        else:\n",
    "            ax[sp].plot(signal[TRAINLENGTH:TRAINLENGTH + TEST,sp], color='k')\n",
    "            ax[sp].plot(preds[TRAINLENGTH:TRAINLENGTH + TEST,sp],\n",
    "                     linestyle='dotted')\n",
    "plt.legend(bbox_to_anchor=(1.11, 2.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "def rank_curve(cplus, tols):\n",
    "    rank_tols = []\n",
    "    for i in tols:\n",
    "        rank_tols.append(rca.rank(cplus, i))\n",
    "    return np.array(rank_tols)\n",
    "\n",
    "K = 3\n",
    "L = 3\n",
    "tols = [1/10**x for x in range(0,30)]\n",
    "plt.figure()\n",
    "for n in size:\n",
    "    mat = np.array(dict_models[str(n)]['Wnet'])\n",
    "    Wr, Wi = rca.get_mats(None, K,n, matrix=mat)\n",
    "    # params [alpha, spectralradius, gamma, ri, rr, loss]\n",
    "    p = dict_models[str(n)]['Params']\n",
    "    a,g = p[0], p[2]\n",
    "    print(f'Alpha {a:2f} --- Gamma {g:2f}')\n",
    "    x0 = np.zeros((n,1))\n",
    "    u0 = np.zeros((K,1))\n",
    "    A = rca.leaky_jacobian(x0, u0, a, g, Wi, Wr)\n",
    "    B = rca.partial_u(x0, u0, a, g, Wi, Wr)\n",
    "    rhoA = np.max(np.abs(rca.eig_spectrum(A)))\n",
    "    Cn = np.nan_to_num(rca.reachable_matrix(A,B))\n",
    "    if(K != 3): # Square Cn\n",
    "        Cn = Cn/np.max(np.abs(rca.eig_spectrum(Cn)))\n",
    "    else:            # Non-square Cn\n",
    "        Cn = Cn/np.max(np.abs(np.linalg.svd(Cn, compute_uv=False)))\n",
    "\n",
    "    rkc = rank_curve(Cn, tols)\n",
    "    v = np.argmax(np.gradient(rkc))\n",
    "    plt.plot(rkc, label=f'N={n}')\n",
    "    plt.vlines([v],0,50, color='k')\n",
    "    ave_rank = (rkc[v]+rkc[v+1])//2\n",
    "    print(f'Ave rank for N={n} is {ave_rank} sr={p[1]:3f} v={v} Tolerance {tols[v]} Rho A {rhoA.round(3)}')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.signal import argrelmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "K = 3\n",
    "v = 3\n",
    "pred = np.array(dict_models[str(30)]['Preds'])\n",
    "start, end= 13900,15000\n",
    "p = dict_models[str(n)]['Params']\n",
    "mat = np.array(dict_models[str(N)]['Wnet'])\n",
    "Wr, Wi = rca.get_mats(None, K,N, matrix=mat)\n",
    "sig_rks = rca.rank_along_trajectory( Wr, Wi, p[0], p[2],\n",
    "                            signal[start:end], N, K, tols[v])\n",
    "pred_rks = rca.rank_along_trajectory( Wr, Wi, p[0], p[2],\n",
    "                            pred[start:end],N, K, tols[v])\n",
    "\n",
    "top,bottom  = 13900,15000\n",
    "mu = np.array([.56018255, .55704211, 23.59124571])\n",
    "\n",
    "gs = gridspec.GridSpec(2, 1)\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "ax1 = fig.add_subplot(gs[0], projection='3d')\n",
    "ax1.plot(signal[14000:,0]+mu[0],\n",
    "         signal[14000:,1]+mu[1],\n",
    "         signal[14000:,2]+mu[2],\n",
    "         c='k', label='$L3D$')\n",
    "ax1.plot(pred[14000:,0]+mu[0],\n",
    "         pred[14000:,1]+mu[1],\n",
    "         pred[14000:,2]+mu[2],\n",
    "         c='r', label='$\\widehat{\\mathbf{y}},$ $N=30$', alpha=.5)\n",
    "ax1.set_xlabel('$v^{(1)}_t$', fontsize=26, color='black')\n",
    "ax1.set_ylabel('$v^{(2)}_t$', fontsize=26, color='black')\n",
    "ax1.set_zlabel('$v^{(3)}_t$', fontsize=26, color='black')\n",
    "ax1.w_xaxis.set_pane_color ((0., 0., 0., 0.))\n",
    "ax1.w_yaxis.set_pane_color ((0., 0., 0., 0.))\n",
    "ax1.w_zaxis.set_pane_color ((0., 0., 0., 0.))\n",
    "ax1.xaxis.labelpad = 20\n",
    "ax1.yaxis.labelpad = 20\n",
    "ax1.zaxis.labelpad = 20\n",
    "ax1.set_zticklabels([10,20,30,40])\n",
    "ax1.legend(fontsize=24,\n",
    "           labelcolor='black',\n",
    "           ncol=2,\n",
    "           handlelength=.5,\n",
    "           borderpad=.2,\n",
    "           borderaxespad=.2)\n",
    "ax1.text(-.2, 1.,310,s='$(\\mathbf{a})$', transform=ax1.transAxes, \n",
    "            size=30, weight='bold')\n",
    "\n",
    "gs1 = gs[1].subgridspec(2,1, hspace=0)\n",
    "ax2 = plt.subplot(gs1[0])\n",
    "ax2.text(-0.2, 1.,s='$(\\mathbf{b})$', transform=ax2.transAxes, \n",
    "            size=30, weight='bold')\n",
    "ax2.plot(signal[top:bottom-500, 0], color='k', label='$v^{(3)}_t$')\n",
    "ax2.plot(pred[top:bottom-500, 0],'coral', ms=3, label = '$\\widehat{y}^{(3)}$')\n",
    "ax2.axvline(100, color='k', linestyle='dashed')\n",
    "ax2.axvline(278, color='r', linestyle='dashed')\n",
    "ax2.set_ylabel('$L3D -\\mu_{L3D}$', labelpad=10)\n",
    "ax2.set_ylim(-25,25)\n",
    "ax2.legend(loc='lower right',\n",
    "           ncol=2 ,\n",
    "           handlelength=.5,\n",
    "           fontsize=22,\n",
    "           borderpad=.2,\n",
    "           borderaxespad=.2)\n",
    "ax2.set_xticks([])\n",
    "\n",
    "ax3 = plt.subplot(gs1[1])\n",
    "ax3.plot([x for x in range(top,bottom-500)],\n",
    "         sig_rks[:-500], color='k',\n",
    "         label='$L3D$', alpha=.8)\n",
    "ax3.plot([x for x in range(top,bottom-500)],\n",
    "         pred_rks[:-500], color= 'coral',\n",
    "         label='$\\widehat{\\mathbf{y}},$ $N=30$', alpha=.6)\n",
    "\n",
    "ax3.set_ylabel('$\\mathrm{rank}(\\mathbf{\\mathcal{C}}_{25};10^{-8})$', labelpad=22)\n",
    "ax3.set_xlabel('Time-step $t$')\n",
    "ax3.text(-0.2, 0,s='$(\\mathbf{c})$', transform=ax2.transAxes, \n",
    "            size=30, weight='bold')\n",
    "ax3.legend(handlelength=.5,\n",
    "           ncol=2,\n",
    "           loc='lower right',\n",
    "           fontsize=22,\n",
    "           borderpad=.2,\n",
    "           borderaxespad=.2)\n",
    "ax3.set_xticks([13900,14000,14100,14200,14300,14400])\n",
    "ax3.set_xticklabels([-100,0,100,200,300,400])\n",
    "\n",
    "#maxradii = np.array(list(set((argrelmax(sigradius)[0]).tolist()) & set((np.where(sigradius>1)[0]).tolist())))\n",
    "ax3.axvline(14000, color='k', linestyle='dashed')\n",
    "ax3.axvline(14178, color='r', linestyle='dashed')\n",
    "#ax3.set_ylim(27,29)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"../PlotNBs/NBoutputs/L3D_lines_combined.pdf\", format='pdf', bbox_inches='tight', pad_inches=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rxy(x,y):\n",
    "    return np.dot(x.T,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "\n",
    "start = TRAINLENGTH - 1000\n",
    "stop = M+FREERUN\n",
    "k_list = [30]\n",
    "fig, ax = plt.subplots(3, figsize=(10,9))\n",
    "for sp in range(3):\n",
    "    for k in k_list:\n",
    "        lengthTC = (stop-start)-k\n",
    "        trainCorr = np.zeros(lengthTC) # 21000-k\n",
    "        pred = np.array(dict_models[str(k)]['Preds'])\n",
    "        for j in range(start,stop-k):\n",
    "            tar = signal[j:j+k,sp].reshape((k,1))\n",
    "            prd = pred[j:j+k,sp].reshape((k,1))\n",
    "            trainCorr[j-start] = Rxy(tar,prd)\n",
    "        minVal = trainCorr[:1000].min()\n",
    "        threshold = minVal*.95\n",
    "        thresholdLoc = np.where(trainCorr[1000:]<threshold)[0][0]\n",
    "        print(f'k-size={k} Trainingset min={minVal.round(3)}  threshold={threshold:.3f}',\n",
    "              f' Location={thresholdLoc}')\n",
    "        #ax[sp].plot(trainCorr[500:1300], color='r', label=f'k={k}')\n",
    "    ax[sp].plot(signal[TRAINLENGTH-500:TRAINLENGTH+TEST,sp], color='k', label='target')\n",
    "    ax[sp].plot(pred[TRAINLENGTH-500:TRAINLENGTH+TEST,sp], label='pred')\n",
    "    ax[sp].axvline(500+thresholdLoc,color='r', label='diverged')\n",
    "    ax[sp].axvline(500,color='k',linestyle='dashed')\n",
    "    ax[sp].set_ylabel('$\\mathbf{R}_{xy}$')\n",
    "    ax[sp].set_xlabel('Time-steps $t$ samples are: Train $[0,1000]$, Test $[1000,1500]$')\n",
    "    ax[sp].set_xticklabels([-500,-250,0,250,500,750])\n",
    "    plt.legend(fontsize=24,bbox_to_anchor=(1.31, 2.05))\n",
    "    ax[0].set_title('Prediction divergence (red lines) by $\\mathbf{R}_{xy}[k]$ for $k=30$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
